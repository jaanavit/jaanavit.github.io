<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A.2-A.4: Image Mosaicing Technical Report</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', sans-serif;
            background: #f5f7fa;
            color: #2c3e50;
            line-height: 1.6;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
        }

        .header {
            background: #1a1a2e;
            color: white;
            padding: 40px 60px;
            border-bottom: 4px solid #0f3460;
        }

        .header h1 {
            font-size: 2.2em;
            font-weight: 600;
            margin-bottom: 8px;
            letter-spacing: -0.5px;
        }

        .header .subtitle {
            font-size: 1.1em;
            color: #b8c5d6;
            font-weight: 400;
        }

        .nav {
            background: #16213e;
            padding: 0 60px;
            border-bottom: 1px solid #0f3460;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .nav ul {
            list-style: none;
            display: flex;
            gap: 30px;
        }

        .nav a {
            display: block;
            color: #b8c5d6;
            text-decoration: none;
            padding: 15px 0;
            font-weight: 500;
            border-bottom: 3px solid transparent;
            transition: all 0.3s;
        }

        .nav a:hover {
            color: white;
            border-bottom-color: #0f3460;
        }

        .content {
            padding: 40px 60px;
        }

        .section {
            margin-bottom: 60px;
        }

        .section-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #1a1a2e;
            margin-bottom: 25px;
            padding-bottom: 12px;
            border-bottom: 2px solid #e8eef5;
        }

        .subsection-title {
            font-size: 1.3em;
            font-weight: 600;
            color: #16213e;
            margin: 30px 0 15px 0;
        }

        .card {
            background: white;
            border: 1px solid #e8eef5;
            border-radius: 8px;
            padding: 30px;
            margin: 20px 0;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.04);
        }

        .matrix-display {
            background: #f8fafb;
            border: 1px solid #d9e2ec;
            border-radius: 6px;
            padding: 30px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
        }

        .matrix-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 12px;
            max-width: 550px;
            margin: 20px auto;
        }

        .matrix-cell {
            background: white;
            padding: 18px;
            text-align: center;
            font-size: 1.05em;
            font-weight: 600;
            color: #1a1a2e;
            border: 1px solid #d9e2ec;
            border-radius: 4px;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }

        .stat-box {
            background: #f8fafb;
            border-left: 4px solid #0f3460;
            padding: 25px;
            border-radius: 4px;
        }

        .stat-value {
            font-size: 2.5em;
            font-weight: 700;
            color: #1a1a2e;
            margin: 8px 0;
        }

        .stat-label {
            font-size: 0.95em;
            color: #64748b;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            font-weight: 500;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            font-size: 0.95em;
        }

        table thead {
            background: #16213e;
            color: white;
        }

        table th {
            padding: 14px 16px;
            text-align: left;
            font-weight: 600;
            font-size: 0.9em;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        table td {
            padding: 12px 16px;
            border-bottom: 1px solid #e8eef5;
        }

        table tbody tr:hover {
            background: #f8fafb;
        }

        .point-id {
            font-weight: 600;
            color: #0f3460;
        }

        .error-low { color: #059669; font-weight: 600; }
        .error-medium { color: #d97706; font-weight: 600; }
        .error-high { color: #dc2626; font-weight: 600; }

        .image-container {
            margin: 30px 0;
            border: 1px solid #e8eef5;
            border-radius: 8px;
            overflow: hidden;
            background: white;
        }

        .image-container img {
            width: 100%;
            display: block;
        }

        .image-caption {
            padding: 15px 20px;
            background: #f8fafb;
            color: #64748b;
            font-size: 0.9em;
            border-top: 1px solid #e8eef5;
        }

        .grid-2 {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(450px, 1fr));
            gap: 30px;
            margin: 25px 0;
        }

        .formula {
            background: #f8fafb;
            border-left: 4px solid #0f3460;
            padding: 20px 25px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            font-size: 1.05em;
            border-radius: 4px;
        }

        .info-box {
            background: #eff6ff;
            border-left: 4px solid #3b82f6;
            padding: 20px 25px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .file-list {
            list-style: none;
            margin: 20px 0;
        }

        .file-list li {
            padding: 12px 0;
            border-bottom: 1px solid #e8eef5;
            display: flex;
            align-items: center;
            gap: 12px;
        }

        .file-list li:last-child {
            border-bottom: none;
        }

        .file-icon {
            width: 24px;
            height: 24px;
            background: #0f3460;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 0.7em;
            font-weight: 700;
        }

        .download-btn {
            background: #0f3460;
            color: white;
            padding: 10px 20px;
            border-radius: 4px;
            text-decoration: none;
            font-weight: 600;
            font-size: 0.9em;
            display: inline-block;
            margin-left: auto;
        }

        .download-btn:hover {
            background: #1a1a2e;
        }

        .methodology {
            background: #f8fafb;
            padding: 30px;
            border-radius: 8px;
            border: 1px solid #e8eef5;
        }

        .methodology ol {
            margin-left: 20px;
            margin-top: 15px;
        }

        .methodology li {
            margin: 12px 0;
            padding-left: 10px;
        }

        .deliverable-section {
            background: #f8fafb;
            border-radius: 8px;
            padding: 25px;
            margin: 20px 0;
            border: 1px solid #e8eef5;
        }

        .deliverable-section h4 {
            color: #16213e;
            margin-bottom: 15px;
            font-size: 1.1em;
        }

        .deliverable-section p {
            color: #64748b;
            margin-bottom: 10px;
            line-height: 1.7;
        }

        @media (max-width: 768px) {
            .header, .content, .nav {
                padding: 20px 30px;
            }

            .section-title {
                font-size: 1.5em;
            }

            .grid-2 {
                grid-template-columns: 1fr;
            }

            .stats-grid {
                grid-template-columns: 1fr;
            }

            table {
                font-size: 0.85em;
            }

            table th, table td {
                padding: 8px 10px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Image Mosaicing Project</h1>
            <p class="subtitle">Part A: Manual Stitching | Part B: Automatic Feature Detection & Matching</p>
        </div>

        <nav class="nav">
            <ul>
                <li><a href="#a2-overview">A.2: Homography</a></li>
                <li><a href="#a3-warping">A.3: Warping</a></li>
                <li><a href="#a4-mosaic">A.4: Mosaic</a></li>
                <li><a href="#b1-harris">B.1: Harris Corners</a></li>
                <li><a href="#b2-descriptors">B.2: Descriptors</a></li>
                <li><a href="#b3-matching">B.3: Matching</a></li>
                <li><a href="#b4-ransac">B.4: RANSAC</a></li>
            </ul>
        </nav>

        <div class="content">
            
            <!-- A.2 SECTION -->
            <section id="a2-overview" class="section">
                <h2 class="section-title">A.2: Recover Homographies</h2>
                <div class="card">
                    <p style="font-size: 1.05em; margin-bottom: 15px;">
                        This section presents the computation and verification of homography matrices for aligning multiple images 
                        of a library interior. The homography H is a 3×3 matrix that relates corresponding points between two images 
                        through a projective transformation.
                    </p>
                    <div class="formula">
                        <strong>Transformation Equation:</strong> p' = H · p<br>
                        where p = [x, y, 1]ᵀ and p' = [x', y', 1]ᵀ are homogeneous coordinates
                    </div>
                </div>

                <h3 class="subsection-title">Methodology</h3>
                <div class="methodology">
                    <h3 style="color: #1a1a2e; margin-bottom: 15px;">System of Linear Equations (Ah = b)</h3>
                    <p>Each point correspondence (x, y) → (x', y') provides two equations:</p>
                    <div class="formula" style="margin: 15px 0;">
                        Equation 1: h₁₁·x + h₁₂·y + h₁₃ - h₃₁·x·x' - h₃₂·y·x' = x'<br>
                        Equation 2: h₂₁·x + h₂₂·y + h₂₃ - h₃₁·x·y' - h₃₂·y·y' = y'
                    </div>
                    
                    <h3 style="color: #1a1a2e; margin: 25px 0 15px 0;">Solution Method</h3>
                    <ol>
                        <li><strong>Point Selection:</strong> Select n ≥ 4 corresponding points between image pairs using interactive GUI</li>
                        <li><strong>System Construction:</strong> Build matrix A (2n × 9) where each row represents one equation</li>
                        <li><strong>SVD Decomposition:</strong> Compute A = U·Σ·Vᵀ</li>
                        <li><strong>Extract Solution:</strong> The homography vector h is the last column of V (null space of A)</li>
                        <li><strong>Normalization:</strong> Reshape h into 3×3 matrix H and normalize so H₃₃ = 1</li>
                    </ol>
                </div>

                <h3 class="subsection-title">Recovered Homography Matrix H₁₂</h3>
                <div class="matrix-display">
                    <div style="text-align: center; margin-bottom: 15px; color: #64748b; font-size: 0.9em;">
                        H₁₂: doe1.png → doe2.png (Image 1 to Image 2)
                    </div>
                    <div class="matrix-grid">
                        <div class="matrix-cell">0.983699</div>
                        <div class="matrix-cell">0.014455</div>
                        <div class="matrix-cell">-280.225</div>
                        
                        <div class="matrix-cell">-0.008591</div>
                        <div class="matrix-cell">0.992651</div>
                        <div class="matrix-cell">8.469</div>
                        
                        <div class="matrix-cell">-0.000011</div>
                        <div class="matrix-cell">0.000007</div>
                        <div class="matrix-cell">1.000000</div>
                    </div>
                </div>

                <h3 class="subsection-title">System of Equations (Ah = b form)</h3>
<div class="info-box">
    <strong>Linear System Construction:</strong> Each point correspondence generates two equations. With 15 point pairs, 
    we construct a system of 30 equations . The system is solved using least squares 
    to find the optimal homography matrix that minimizes reprojection error across all points.
</div>

<div class="card">
    <div style="font-family: 'Courier New', monospace; font-size: 0.9em; line-height: 1.6;">
        <div style="margin-bottom: 15px; color: #64748b;">
            Where <strong>h</strong> = [h₁₁, h₁₂, h₁₃, h₂₁, h₂₂, h₂₃, h₃₁, h₃₂, h₃₃]ᵀ<br>
            And we set h₃₃ = 1 for scaling
        </div>
        
        <div style="background: #f8fafb; padding: 20px; border-radius: 4px; margin-bottom: 15px;">
            <strong style="color: #0f3460;">Point 1:</strong> (933.7, 637.6) → (649.7, 638.5)
            <div style="margin-left: 20px; margin-top: 8px; color: #334155;">
                Equation 1: 933.7·h₁₁ + 637.6·h₁₂ + h₁₃ - 606641.5·h₃₁ - 414243.9·h₃₂ = 649.7<br>
                Equation 2: 933.7·h₂₁ + 637.6·h₂₂ + h₂₃ - 596193.9·h₃₁ - 407109.7·h₃₂ = 638.5
            </div>
        </div>
        
        <div style="background: #f8fafb; padding: 20px; border-radius: 4px; margin-bottom: 15px;">
            <strong style="color: #0f3460;">Point 2:</strong> (1225.3, 866.4) → (959.5, 855.8)
            <div style="margin-left: 20px; margin-top: 8px; color: #334155;">
                Equation 1: 1225.3·h₁₁ + 866.4·h₁₂ + h₁₃ - 1175679.0·h₃₁ - 831301.7·h₃₂ = 959.5<br>
                Equation 2: 1225.3·h₂₁ + 866.4·h₂₂ + h₂₃ - 1048603.8·h₃₁ - 741449.1·h₃₂ = 855.8
            </div>
        </div>
        
        <div style="background: #f8fafb; padding: 20px; border-radius: 4px; margin-bottom: 15px;">
            <strong style="color: #0f3460;">Point 3:</strong> (1603.3, 853.8) → (1289.0, 860.0)
            <div style="margin-left: 20px; margin-top: 8px; color: #334155;">
                Equation 1: 1603.3·h₁₁ + 853.8·h₁₂ + h₁₃ - 2066634.5·h₃₁ - 1100530.7·h₃₂ = 1289.0<br>
                Equation 2: 1603.3·h₂₁ + 853.8·h₂₂ + h₂₃ - 1378897.1·h₃₁ - 734294.6·h₃₂ = 860.0
            </div>
        </div>
        
        <div style="text-align: center; padding: 15px; color: #64748b; font-style: italic;">
            ... and 12 more point(s)<br>
            <strong style="color: #0f3460; font-size: 1.1em; margin-top: 8px; display: inline-block;">Total: 30 equations</strong>
        </div>
    </div>
</div>

<h3 class="subsection-title">Point Correspondence Visualization</h3>
    <div class="image-container">
        <img src="correspondences_visualization.png" alt="Point correspondences between images">
        <div class="image-caption">
            Figure 1: Visualization of 15 corresponding points selected between Image 1 (left) and Image 2 (right). 
            Matching numbers indicate the same physical feature in both images.
        </div>
    </div>


                <h3 class="subsection-title">Point Correspondence Data</h3>
<div class="grid-2">
    <div class="card">
        <h3 style="color: #1a1a2e; margin-bottom: 20px;">Image 1 Points (doe1.png)</h3>
        <table>
            <thead>
                <tr>
                    <th>Point</th>
                    <th>X Coordinate</th>
                    <th>Y Coordinate</th>
                </tr>
            </thead>
            <tbody>
                <tr><td class="point-id">1</td><td>933.70</td><td>637.60</td></tr>
                <tr><td class="point-id">2</td><td>1225.30</td><td>866.40</td></tr>
                <tr><td class="point-id">3</td><td>1603.30</td><td>853.80</td></tr>
                <tr><td class="point-id">4</td><td>1604.70</td><td>965.40</td></tr>
                <tr><td class="point-id">5</td><td>1239.20</td><td>976.60</td></tr>
                <tr><td class="point-id">6</td><td>1462.40</td><td>682.20</td></tr>
                <tr><td class="point-id">7</td><td>1482.00</td><td>612.50</td></tr>
                <tr><td class="point-id">8</td><td>1289.40</td><td>510.60</td></tr>
                <tr><td class="point-id">9</td><td>1262.90</td><td>433.90</td></tr>
                <tr><td class="point-id">10</td><td>1131.80</td><td>1319.80</td></tr>
                <tr><td class="point-id">11</td><td>1894.90</td><td>1315.60</td></tr>
                <tr><td class="point-id">12</td><td>1242.00</td><td>1213.70</td></tr>
                <tr><td class="point-id">13</td><td>787.20</td><td>918.00</td></tr>
                <tr><td class="point-id">14</td><td>651.90</td><td>415.80</td></tr>
                <tr><td class="point-id">15</td><td>644.90</td><td>902.60</td></tr>
            </tbody>
        </table>
    </div>

    <div class="card">
        <h3 style="color: #1a1a2e; margin-bottom: 20px;">Image 2 Points (doe2.png)</h3>
        <table>
            <thead>
                <tr>
                    <th>Point</th>
                    <th>X Coordinate</th>
                    <th>Y Coordinate</th>
                </tr>
            </thead>
            <tbody>
                <tr><td class="point-id">1</td><td>649.72</td><td>638.53</td></tr>
                <tr><td class="point-id">2</td><td>959.53</td><td>855.82</td></tr>
                <tr><td class="point-id">3</td><td>1288.97</td><td>860.02</td></tr>
                <tr><td class="point-id">4</td><td>1297.38</td><td>972.17</td></tr>
                <tr><td class="point-id">5</td><td>948.31</td><td>967.97</td></tr>
                <tr><td class="point-id">6</td><td>1157.19</td><td>696.01</td></tr>
                <tr><td class="point-id">7</td><td>1195.04</td><td>623.11</td></tr>
                <tr><td class="point-id">8</td><td>1025.42</td><td>499.74</td></tr>
                <tr><td class="point-id">9</td><td>997.38</td><td>446.47</td></tr>
                <tr><td class="point-id">10</td><td>829.16</td><td>1331.05</td></tr>
                <tr><td class="point-id">11</td><td>1516.07</td><td>1284.79</td></tr>
                <tr><td class="point-id">12</td><td>948.31</td><td>1200.68</td></tr>
                <tr><td class="point-id">13</td><td>481.49</td><td>911.89</td></tr>
                <tr><td class="point-id">14</td><td>352.52</td><td>366.57</td></tr>
                <tr><td class="point-id">15</td><td>323.08</td><td>896.47</td></tr>
            </tbody>
        </table>
    </div>
</div>

    

            </section>

            <!-- A.3 SECTION -->
            <section id="a3-warping" class="section">
                <h2 class="section-title">A.3: Warp the Images</h2>
                
                <div class="card">
                    <p style="font-size: 1.05em; margin-bottom: 15px;">
                        This section demonstrates image warping using inverse homography transformation with two interpolation methods: 
                        nearest neighbor and bilinear interpolation. We test the implementation through rectification of rectangular objects 
                        photographed at oblique angles.
                    </p>
                </div>

                <h3 class="subsection-title">Implementation Details</h3>
                <div class="methodology">
                    <h4>Inverse Warping Algorithm</h4>
                    <p style="margin: 15px 0;">To avoid holes in the output image, we use inverse warping:</p>
                    <ol>
                        <li>For each pixel (x', y') in the output image</li>
                        <li>Apply H⁻¹ to find source location (x, y) = H⁻¹ · (x', y', 1)</li>
                        <li>Use interpolation to sample the source image at (x, y)</li>
                        <li>Write interpolated value to output pixel (x', y')</li>
                    </ol>

                    <h4 style="margin-top: 25px;">Interpolation Methods</h4>
                    <p style="margin: 15px 0;"><strong>Nearest Neighbor:</strong> Round (x, y) to nearest integer coordinates and copy that pixel value</p>
                    <p style="margin: 15px 0;"><strong>Bilinear Interpolation:</strong> Compute weighted average of the four neighboring pixels using distance-based weights</p>
                    <div class="formula" style="margin: 15px 0;">
                        I(x,y) = I₀₀(1-wx)(1-wy) + I₁₀·wx(1-wy) + I₀₁(1-wx)·wy + I₁₁·wx·wy
                    </div>
                </div>

                <h3 class="subsection-title">Rectification Example 1: Door</h3>
                <div class="deliverable-section">
                    <p><strong>Objective:</strong> Remove perspective distortion from a door photographed at an angle</p>
                    <p><strong>Method:</strong> Map the four corners of the trapezoidal door to a perfect rectangle with standard door proportions (7:3 aspect ratio)</p>
                    <p><strong>Result:</strong> Frontal view as if the camera were perpendicular to the door surface</p>
                </div>

                <div class="image-container">
                    <img src="door_rectification_result.png" alt="Door rectification results">
                    <div class="image-caption">
                        Figure 2: Door rectification showing (left) original image with selected quadrilateral, 
                        (center) nearest neighbor result, (right) bilinear interpolation result.
                    </div>
                </div>

                <h3 class="subsection-title">Rectification Example 2: Sign/Poster</h3>
                <div class="deliverable-section">
                    <p><strong>Objective:</strong> Rectify a rectangular sign or poster viewed from an oblique angle</p>
                    <p><strong>Method:</strong> User selects four corners of the sign, system computes homography to map to rectangle</p>
                    <p><strong>Result:</strong> Straight-on view with preserved aspect ratio, readable text</p>
                </div>

                <div class="image-container">
                    <img src="sign_rectification_result.png" alt="Sign rectification results">
                    <div class="image-caption">
                        Figure 3: Sign/poster rectification demonstrating perspective removal. The rectified image shows 
                        parallel edges and preserved right angles.
                    </div>
                </div>

                <!-- A.4 SECTION -->
                <section id="a4-mosaic" class="section">
                    <h2 class="section-title">A.4: Blend the Images into a Mosaic</h2>
                    
                    <div class="card">
                        <h3 style="color: #1a1a2e; margin-bottom: 20px;">Implementation Overview</h3>
                        <p style="font-size: 1.05em; margin-bottom: 15px;">
                            This section demonstrates the creation of seamless panoramic mosaics by warping multiple images to a common 
                            reference frame and blending them using two approaches: <strong>simple weighted averaging</strong> and 
                            <strong>Laplacian pyramid blending</strong>. Three image pairs were processed: library interior (doe1+doe2), 
                            food photography (mochi1+mochi2), and clothing display (clothes1+clothes2).
                        </p>
                        
                        <h4 style="color: #16213e; margin: 25px 0 15px 0;">My Blending Procedure</h4>
                        <div class="methodology">
                            <p><strong>Step 1: Homography Computation</strong></p>
                            <ul style="margin-left: 25px; margin-top: 10px;">
                                <li>Selected 6-9 corresponding points between each image pair using interactive GUI</li>
                                <li>Computed homography matrix H using SVD-based least squares (A.2 implementation)</li>
                                <li>One image designated as reference frame (stays unwarped), other warps to align</li>
                            </ul>
                            
                            <p style="margin-top: 20px;"><strong>Step 2: Mosaic Size Determination (One-Shot Approach)</strong></p>
                            <ul style="margin-left: 25px; margin-top: 10px;">
                                <li>Transformed all four corners of each image through its homography</li>
                                <li>Computed bounding box containing all transformed corners</li>
                                <li>Determined final mosaic dimensions [x_min:x_max, y_min:y_max]</li>
                            </ul>
                            
                            <p style="margin-top: 20px;"><strong>Step 3: Image Warping</strong></p>
                            <ul style="margin-left: 25px; margin-top: 10px;">
                                <li>Warped first image using computed homography and bilinear interpolation (A.3 implementation)</li>
                                <li>Left reference image unwarped (identity transformation)</li>
                                <li>Applied translation offsets to position images correctly in mosaic coordinates</li>
                            </ul>
                            
                            <p style="margin-top: 20px;"><strong>Step 4: Weight Map Creation (Alpha Channels)</strong></p>
                            <ul style="margin-left: 25px; margin-top: 10px;">
                                <li>Created distance-based weight maps using cv2.distanceTransform</li>
                                <li>Weight = 1.0 at image center, falls off to 0.0 at edges</li>
                                <li>Warped weight maps alongside images for proper alignment</li>
                            </ul>
                            
                            <p style="margin-top: 20px;"><strong>Step 5A: Simple Weighted Averaging</strong></p>
                            <ul style="margin-left: 25px; margin-top: 10px;">
                                <li>For each pixel: pixel_value = (im1 × w1 + im2 × w2) / (w1 + w2)</li>
                                <li>Single-scale blending across all frequencies</li>
                                <li>Fast but may show ghosting in high-frequency details</li>
                            </ul>
                            
                            <p style="margin-top: 20px;"><strong>Step 5B: Laplacian Pyramid Blending </strong></p>
                            <ul style="margin-left: 25px; margin-top: 10px;">
                                <li>Built 6-level Laplacian pyramids for both warped images</li>
                                <li>Built 6-level Gaussian pyramids for weight masks</li>
                                <li>Blended images independently at each pyramid level using weights</li>
                                <li>Reconstructed final mosaic by upsampling and adding Laplacian details</li>
                                <li>Multi-scale approach: high frequencies use sharper transitions, low frequencies use smoother blending</li>
                            </ul>
                        </div>
                    </div>
    
                    <!-- MOSAIC 1: LIBRARY -->
                    <h3 class="subsection-title">Mosaic 1: Library Panorama (doe1 + doe2)</h3>
                    <div class="image-container">
                        <img src="library_visualization.png" alt="Library mosaic - source images and result">
                        <div class="image-caption">
                            Figure 4: Library panorama showing (top) source images doe1 and doe2, (bottom) simple weighted averagemosaic
                        </div>
                    </div>
    
                    <div class="image-container">
                        <img src="library_comparison.png" alt="Library blending method comparison">
                        <div class="image-caption">
                            Figure 5: Blending method comparison for library panorama. Top row: source images. Bottom row: simple weighted averaging (left) vs Laplacian pyramid (right). 
                        </div>
                    </div>
    
    
                    <!-- MOSAIC 2: MOCHI -->
                    <h3 class="subsection-title">Mosaic 2: Mochi Food Photography (mochi1 + mochi2)</h3>
                    <div class="image-container">
                        <img src="mochi_visualization.png" alt="Mochi mosaic - source images and result">
                        <div class="image-caption">
                            Figure 7: Mochi panorama showing (top) source images mochi1 and mochi2, (bottom) simple weighted average mosaic
                        </div>
                    </div>
    
                    <div class="image-container">
                        <img src="mochi_comparison.png" alt="Mochi blending method comparison">
                        <div class="image-caption">
                            Figure 8: Blending method comparison for mochi panorama. Laplacian pyramid blending (right). Simple averaging (left).
                        </div>
                    </div>
    
                    <!-- MOSAIC 3: CLOTHES -->
                    <h3 class="subsection-title">Mosaic 3: Clothing Display (clothes1 + clothes2)</h3>
                    <div class="image-container">
                        <img src="clothes_visualization.png" alt="Clothes mosaic - source images and result">
                        <div class="image-caption">
                            Figure 10: Clothing display panorama showing (top) source images clothes1 and clothes2, (bottom) simple weighted average mosaic
                        </div>
                    </div>
    
                    <div class="image-container">
                        <img src="clothes_comparison.png" alt="Clothes blending method comparison">
                        <div class="image-caption">
                            Figure 11: Blending method comparison for clothing display. Laplacian pyramid approach (right), simple averaging (left).
                        </div>
                    </div>
                </section>
                <section id="b1-harris" class="section">
                    <h2 class="section-title">B.1: Harris Corner Detection & ANMS</h2>
                    
                    <div class="card">
                        <p style="font-size: 1.05em; margin-bottom: 15px;">
                            This section implements the Harris Interest Point Detector and Adaptive Non-Maximal Suppression (ANMS) 
                            for selecting well-distributed corner features across an image. These corners serve as key points for 
                            establishing correspondences between images in automatic panorama stitching.
                        </p>
                        <div class="formula">
                            <strong>Harris Response:</strong> R = det(M) - k × trace(M)²<br>
                            where M is the structure tensor (second moment matrix), k = 0.04
                        </div>
                    </div>
    
                    <h3 class="subsection-title">Implementation Details</h3>
                    <div class="methodology">
                        <h4>Harris Corner Detection Algorithm</h4>
                        <ol>
                            <li><strong>Compute Image Gradients:</strong> Calculate I<sub>x</sub> and I<sub>y</sub> using derivative filters</li>
                            <li><strong>Structure Tensor:</strong> Compute products I<sub>x</sub>², I<sub>y</sub>², and I<sub>x</sub>I<sub>y</sub>, then smooth with Gaussian (σ=1)</li>
                            <li><strong>Harris Response:</strong> Calculate R = det(M) - k·trace(M)² where k=0.04</li>
                            <li><strong>Non-Maximum Suppression:</strong> Find local maxima in 3×3 neighborhoods</li>
                            <li><strong>Edge Discard:</strong> Remove corners within 20 pixels of image boundary</li>
                        </ol>
    
                        <h4 style="margin-top: 25px;">Adaptive Non-Maximal Suppression (ANMS)</h4>
                        <p style="margin: 15px 0;">
                            ANMS improves spatial distribution of detected corners by selecting those with large suppression radii:
                        </p>
                        <ol>
                            <li>For each corner i, find minimum distance r<sub>i</sub> to any corner j where strength(j) > 0.9 × strength(i)</li>
                            <li>Sort corners by suppression radius r<sub>i</sub> in descending order</li>
                            <li>Select top N corners (typically 500) with largest radii</li>
                            <li>Result: evenly distributed corners rather than clustered in high-texture regions</li>
                        </ol>
                    </div>
    
    
                    <div class="image-container">
                        <img src="results/door_harris_anms_comparison.png" alt="Harris corners comparison">
                        <div class="image-caption">
                            Figure 7: Comparison of Harris corner detection. Left: All 48313 detected corners showing clustering 
                            in high-texture areas (door frame, edges). Right: 500 ANMS-selected corners with improved spatial distribution 
                            across the entire image. Red crosses mark corner locations with yellow circle highlights.
                        </div>
                    </div>
            </section>
               <!-- B.2 SECTION: FEATURE DESCRIPTOR EXTRACTION -->
            <section id="b2-descriptors" class="section">
                <h2 class="section-title">B.2: Feature Descriptor Extraction</h2>
                
                <div class="card">
                    <p style="font-size: 1.05em; margin-bottom: 15px;">
                        This section implements feature descriptor extraction for each detected corner point. We extract 
                        axis-aligned 8×8 patches sampled from a larger 40×40 window to create robust, blurred descriptors 
                        that are invariant to affine intensity changes through bias/gain normalization.
                    </p>
                    <div class="formula">
                        <strong>Descriptor Normalization:</strong> d' = (d - μ) / σ<br>
                        where μ is mean, σ is standard deviation (makes descriptor invariant to bias/gain)
                    </div>
                </div>

                <h3 class="subsection-title">Implementation Details</h3>
                <div class="methodology">
                    <h4>Feature Descriptor Extraction Algorithm</h4>
                    <ol>
                        <li><strong>Extract 40×40 Window:</strong> Center window around each corner keypoint location</li>
                        <li><strong>Gaussian Blur:</strong> Apply Gaussian filter with σ=2.5 to avoid aliasing during downsampling</li>
                        <li><strong>Sample 8×8 Grid:</strong> Sample at 5-pixel spacing to create 8×8 descriptor patch</li>
                        <li><strong>Flatten to Vector:</strong> Convert 8×8 patch to 64-dimensional feature vector</li>
                        <li><strong>Bias/Gain Normalization:</strong> Normalize to mean=0 and std=1 for illumination invariance</li>
                    </ol>

                    <h4 style="margin-top: 25px;">Why 40×40 Window with 5-Pixel Sampling?</h4>
                    <p style="margin: 15px 0;">
                        The larger 40×40 window with coarse sampling provides several advantages:
                    </p>
                    <ul>
                        <li><strong>Robustness to localization error:</strong> Low-frequency sampling makes features less sensitive to small position errors</li>
                        <li><strong>Avoids aliasing:</strong> Gaussian smoothing before downsampling prevents high-frequency artifacts</li>
                        <li><strong>Captures broader context:</strong> Larger window includes more surrounding structure for better matching</li>
                        <li><strong>Computational efficiency:</strong> 64-dimensional vectors are compact yet distinctive</li>
                    </ul>
                </div>

                <h3 class="subsection-title">Example 1: Library Image (doe1.png)</h3>
                
                <div class="image-container">
                    <img src="results/doe1_feature_locations.png" alt="Doe1 feature locations">
                    <div class="image-caption">
                        Figure 10: Feature locations with color-coded 40×40 extraction windows on library image. 
                        Each colored square shows the window from which the corresponding descriptor is extracted.
                    </div>
                </div>

                <div class="image-container">
                    <img src="results/doe1_descriptor_grid.png" alt="Doe1 descriptor grid">
                    <div class="image-caption">
                        Figure 11: Grid of 9 extracted 8×8 descriptors from library image after bias/gain normalization. 
                    </div>
                </div>

                <h3 class="subsection-title">Example 2: Mochi Image (mochi1.png)</h3>
                
                <div class="image-container">
                    <img src="results/mochi1_feature_locations.png" alt="Mochi1 feature locations">
                    <div class="image-caption">
                        Figure 12: Feature locations with color-coded 40×40 extraction windows on mochi image. 
                        The diverse feature locations capture both texture details on the mochi and background elements.
                    </div>
                </div>

                <div class="image-container">
                    <img src="results/mochi1_descriptor_grid.png" alt="Mochi1 descriptor grid">
                    <div class="image-caption">
                        Figure 13: Grid of 9 extracted 8×8 descriptors from mochi image. 
                    </div>
                </div>
            </section>
    <!-- B.3 SECTION: FEATURE MATCHING -->
    <section id="b3-matching" class="section">
        <h2 class="section-title">B.3: Feature Matching</h2>
        
        <div class="card">
            <p style="font-size: 1.05em; margin-bottom: 15px;">
                This section implements feature matching between image pairs using Lowe's ratio test. 
                For each feature in the first image, we find its two nearest neighbors in the second image 
                and accept the match only if the ratio of distances is sufficiently small, indicating a 
                distinctive and reliable correspondence.
            </p>
            <div class="formula">
                <strong>Lowe's Ratio Test:</strong> Accept match if e₁₋ₙₙ / e₂₋ₙₙ < 0.8<br>
                where e₁₋ₙₙ is distance to first nearest neighbor, e₂₋ₙₙ is distance to second nearest neighbor
            </div>
        </div>

        <h3 class="subsection-title">Implementation Details</h3>
        <div class="methodology">
            <h4>Feature Matching Algorithm</h4>
            <ol>
                <li><strong>Compute Pairwise Distances:</strong> Calculate L2 distances between all descriptor pairs using efficient vectorized operations</li>
                <li><strong>Find Nearest Neighbors:</strong> For each descriptor in image 1, identify the two closest descriptors in image 2</li>
                <li><strong>Lowe's Ratio Test:</strong> Accept match if ratio = d₁/d₂ < 0.8, where d₁ and d₂ are distances to first and second nearest neighbors</li>
                <li><strong>Confidence Score:</strong> Compute match confidence as (1 - ratio), with higher values indicating more distinctive matches</li>
                <li><strong>Filter Matches:</strong> Retain only matches passing the ratio threshold</li>
            </ol>

            <h4 style="margin-top: 25px;">Why Lowe's Ratio Test?</h4>
            <p style="margin: 15px 0;">
                Lowe's ratio test is more effective than simple distance thresholding because:
            </p>
            <ul>
                <li><strong>Scale-invariant:</strong> The ratio is independent of the absolute scale of descriptor distances</li>
                <li><strong>Better separation:</strong> Correct matches have substantially lower ratios than incorrect matches (see paper Figure 6b)</li>
                <li><strong>Adapts to feature space:</strong> Threshold works across different regions of feature space with varying error magnitudes</li>
                <li><strong>Reduces ambiguity:</strong> Rejects matches where multiple features look similar, keeping only distinctive correspondences</li>
            </ul>
        </div>

        <div class="info-box">
            <strong>Note:</strong> The ratio threshold of 0.8 is chosen based on Figure 6b in the Multi-Image Matching paper, 
            which shows good separation between correct and incorrect matches at this value. Lower thresholds (e.g., 0.7) 
            increase precision but reduce the number of matches, while higher thresholds (e.g., 0.9) increase recall 
            but allow more false positives.
        </div>

        <h3 class="subsection-title">Match Results: Library Panorama (doe1 → doe2)</h3>
        <div class="image-container">
            <img src="results/doe1_matches.png" alt="Doe feature matches">
            <div class="image-caption">
                Figure 14: Feature matches between library images (doe1 and doe2). Each colored line connects a matched 
                feature pair, with 50 matches displayed out of ~200 total matches found. The diverse spatial distribution 
                of matches across the entire image ensures robust alignment. Yellow circles mark feature locations.
            </div>
        </div>

        <h3 class="subsection-title">Match Results: Mochi Food Photography (mochi1 → mochi2)</h3>
        <div class="image-container">
            <img src="results/mochi1_matches.png" alt="Mochi feature matches">
            <div class="image-caption">
                Figure 15: Feature matches between mochi images. Despite the challenging textured surface and similar 
                appearance of multiple mochi pieces, the descriptor matching successfully identifies ~180 reliable 
                correspondences. Notice matches on both the mochi themselves and the background plate.
            </div>
        </div>

        <h3 class="subsection-title">Match Results: Clothing Display (clothes1 → clothes2)</h3>
        <div class="image-container">
            <img src="results/clothes1_matches.png" alt="Clothes feature matches">
            <div class="image-caption">
                Figure 16: Feature matches between clothing display images. The algorithm finds ~220 matches across 
                various regions including fabric textures, edges, and background elements. The high match rate indicates 
                good overlap and feature repeatability between the two views.
            </div>
        </div>

        <h3 class="subsection-title">Comparison Across Image Pairs</h3>
        <div class="grid-2">
            <div class="deliverable-section">
                <h4>Library (doe1 + doe2)</h4>
                <p><strong>Matches Found:</strong> ~237 matches<br>
                <strong>Match Rate:</strong> 47.5%<br>
                <strong>Characteristics:</strong> Strong geometric features (edges, corners) from architectural elements 
                provide highly distinctive descriptors. Excellent spatial distribution across the scene.</p>
            </div>
            <div class="deliverable-section">
                <h4>Mochi (mochi1 + mochi2)</h4>
                <p><strong>Matches Found:</strong> ~275 matches<br>
                <strong>Match Rate:</strong> 55%<br>
                <strong>Characteristics:</strong> Challenging due to repetitive textures and similar-looking objects. 
                Matches concentrate on distinctive features and background elements.</p>
            </div>
            <div class="deliverable-section">
                <h4>Clothes (clothes1 + clothes2)</h4>
                <p><strong>Matches Found:</strong> ~266 matches<br>
                <strong>Match Rate:</strong> 53.2%<br>
                <strong>Characteristics:</strong> Rich texture from fabric patterns provides many distinctive features. 
                High match rate suggests good overlap and feature repeatability.</p>
            </div>
            <div class="deliverable-section">
                <h4>Overall Performance</h4>
                <p><strong>Success Factors:</strong> Bias/gain normalization handles lighting variations well. 
                The 40×40 window sampling provides robustness to small localization errors. Lowe's ratio test 
                effectively filters ambiguous matches, maintaining high precision across diverse scene types.</p>
            </div>
        </div>

    </section>
     <!-- B.4 SECTION: RANSAC FOR ROBUST HOMOGRAPHY -->
     <section id="b4-ransac" class="section">
        <h2 class="section-title">B.4: RANSAC for Robust Homography Estimation</h2>
        
        <div class="card">
            <p style="font-size: 1.05em; margin-bottom: 15px;">
                This section implements 4-point RANSAC (Random Sample Consensus) to compute robust homography 
                estimates from feature matches. RANSAC filters outliers by iteratively fitting models to random 
                subsets of matches and selecting the model with the most inliers, enabling accurate alignment 
                even when many matches are incorrect.
            </p>
            <div class="formula">
                <strong>RANSAC Algorithm:</strong> Repeat N times: sample 4 matches → compute H → count inliers<br>
                Return H with most inliers (points where ||H·p - p'|| < threshold)
            </div>
        </div>

        <h3 class="subsection-title">Implementation Details</h3>
        <div class="methodology">
            <h4>4-Point RANSAC Algorithm</h4>
            <ol>
                <li><strong>Random Sampling:</strong> Randomly select 4 point correspondences from all matches</li>
                <li><strong>Compute Homography:</strong> Solve for 3×3 matrix H using SVD on the 4-point system (from Part A.2)</li>
                <li><strong>Count Inliers:</strong> Transform all source points using H and count those within threshold distance (5 pixels) of destinations</li>
                <li><strong>Update Best Model:</strong> If current model has more inliers than previous best, save it</li>
                <li><strong>Iterate:</strong> Repeat steps 1-4 for 5000 iterations to ensure high probability of finding good model</li>
                <li><strong>Refinement:</strong> Recompute H using all inliers from best model for improved accuracy</li>
            </ol>

            <h4 style="margin-top: 25px;">Why RANSAC?</h4>
            <p style="margin: 15px 0;">
                RANSAC is essential for real-world image stitching because:
            </p>
            <ul>
                <li><strong>Handles outliers:</strong> Lowe's ratio test reduces but doesn't eliminate incorrect matches; RANSAC filters remaining outliers</li>
                <li><strong>Robust to noise:</strong> Works well even when 20-40% of matches are incorrect</li>
                <li><strong>No manual tuning:</strong> Automatically separates inliers from outliers based on geometric consistency</li>
                <li><strong>Probabilistic guarantee:</strong> With enough iterations, very high probability of finding correct model</li>
            </ul>

            <h4 style="margin-top: 25px;">RANSAC Parameters</h4>
            <ul>
                <li><strong>Iterations:</strong> 5000 (ensures >99.9% probability of sampling 4 good points)</li>
                <li><strong>Inlier threshold:</strong> 5 pixels (reasonable for typical image resolution and alignment accuracy)</li>
                <li><strong>Minimum points:</strong> 4 (minimum needed to solve for homography's 8 degrees of freedom)</li>
            </ul>
        </div>


        <h3 class="subsection-title">Result 1: Library Panorama (doe1 + doe2)</h3>
        
        <div class="image-container">
            <img src="results/doe_ransac_inliers.png" alt="Doe RANSAC inliers">
            <div class="image-caption">
                Figure 17: RANSAC results for library panorama. Green lines show 168 inliers (84% of matches) 
                that are geometrically consistent with the computed homography. Red lines show 32 outliers 
                rejected by RANSAC. The high inlier ratio indicates excellent match quality.
            </div>
        </div>

        <div class="image-container">
            <img src="results/doe_automatic_mosaic.png" alt="Doe automatic mosaic">
            <div class="image-caption">
                Figure 18: Automatic mosaic created using RANSAC-estimated homography. The alignment is precise 
                with no visible ghosting or misalignment artifacts. Distance-based blending provides smooth 
                transitions in the overlap region.
            </div>
        </div>

        <div class="image-container">
            <img src="results/doe_manual_vs_automatic.png" alt="Doe manual vs automatic comparison">
            <div class="image-caption">
                Figure 19: Comparison of manual (Part A) vs automatic (Part B) stitching for library panorama. 
                Top: source images. Middle: manual stitching with hand-selected correspondences. Bottom: automatic 
                stitching with RANSAC. Both produce high-quality results, validating the automatic pipeline.
            </div>
        </div>

        <h3 class="subsection-title">Result 2: Mochi Food Photography (mochi1 + mochi2)</h3>
        
        <div class="image-container">
            <img src="results/mochi_ransac_inliers.png" alt="Mochi RANSAC inliers">
            <div class="image-caption">
                Figure 20: RANSAC results for mochi panorama. Green lines show 142 inliers (79% of matches). 
                The lower inlier ratio compared to the library scene reflects the challenging repetitive textures, 
                but RANSAC still successfully identifies geometrically consistent matches.
            </div>
        </div>

        <div class="image-container">
            <img src="results/mochi_automatic_mosaic.png" alt="Mochi automatic mosaic">
            <div class="image-caption">
                Figure 21: Automatic mochi mosaic. Despite the challenging scene with repetitive patterns and 
                similar-looking objects, the RANSAC-based pipeline produces accurate alignment. Individual mochi 
                pieces align seamlessly across the stitch boundary.
            </div>
        </div>

        <div class="image-container">
            <img src="results/mochi_manual_vs_automatic.png" alt="Mochi manual vs automatic comparison">
            <div class="image-caption">
                Figure 22: Manual vs automatic comparison for mochi panorama. The automatic method successfully 
                handles this challenging scene with repetitive textures, producing results comparable to manual 
                correspondence selection.
            </div>
        </div>

        <h3 class="subsection-title">Result 3: Clothing Display (clothes1 + clothes2)</h3>
        
        <div class="image-container">
            <img src="results/clothes_ransac_inliers.png" alt="Clothes RANSAC inliers">
            <div class="image-caption">
                Figure 23: RANSAC results for clothing display. Green lines show 183 inliers (83% of matches). 
                The fabric textures provide many distinctive features, resulting in abundant reliable matches 
                and high inlier ratio.
            </div>
        </div>

        <div class="image-container">
            <img src="results/clothes_automatic_mosaic.png" alt="Clothes automatic mosaic">
            <div class="image-caption">
                Figure 24: Automatic clothing display mosaic. The rich texture from fabric patterns enables 
                excellent alignment. Complex patterns like stripes and prints align precisely across the 
                stitched boundary.
            </div>
        </div>

        <div class="image-container">
            <img src="results/clothes_manual_vs_automatic.png" alt="Clothes manual vs automatic comparison">
            <div class="image-caption">
                Figure 25: Manual vs automatic comparison for clothing display. The automatic pipeline matches 
                or exceeds manual stitching quality, demonstrating robust performance across diverse scene types.
            </div>
        </div>

    </section>

</div>
</div>
</body>
</html>